{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import openai\n",
    "import langchain\n",
    "import pinecone\n",
    "# from langchain.document_loaders import PyPDFDirectoryLoader \n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader   # to load documents like pdf etc\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # to split text into chunks\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings  # to convert these chunks into vector form\n",
    "from langchain.vectorstores import Pinecone     # here we need a db to store these vectors\n",
    "from langchain.llms import OpenAI       # then we will need to use llm models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets read the document\n",
    "def read_doc(directory):  # here we can also give the file name directly.\n",
    "    file_loader = PyPDFDirectoryLoader(directory) # through this path it will go to specified directory and load the file.\n",
    "    documents = file_loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = read_doc('documents/')\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upto this we just load the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing the document into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(docs, chunk_size = 800, chunk_overlap = 50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, chunk_overlap = chunk_overlap)\n",
    "    doc = text_splitter.split_documents(docs)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = chunk_data(docs=doc)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings technique of OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\llm_by_krish3\\venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x00000260866F0510>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x00000260866F7990>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(api_key = os.environ['OPENAI_API_KEY'])\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example of we want to see how embedding works\n",
    "vectores = embeddings.embed_query(\"How are you?\")\n",
    "len(vectores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"pinecone-client[grpc]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key='aa77f6f7-bea1-4157-bfa7-a874668f45d5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search vector DB in pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"langchainvector\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud='aws', \n",
    "            region='us-east-1'\n",
    "        ) \n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PodSpec' from 'pinecone.grpc' (d:\\llm_by_krish3\\venv\\Lib\\site-packages\\pinecone\\grpc\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpinecone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrpc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PineconeGRPC \u001b[38;5;28;01mas\u001b[39;00m Pinecone, PodSpec\n\u001b[0;32m      3\u001b[0m pc \u001b[38;5;241m=\u001b[39m Pinecone(api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maa77f6f7-bea1-4157-bfa7-a874668f45d5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m pc\u001b[38;5;241m.\u001b[39mcreate_index(\n\u001b[0;32m      6\u001b[0m   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchainvector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m   dimension\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1536\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m   )\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'PodSpec' from 'pinecone.grpc' (d:\\llm_by_krish3\\venv\\Lib\\site-packages\\pinecone\\grpc\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone, PodSpec\n",
    "\n",
    "pc = Pinecone(api_key=\"aa77f6f7-bea1-4157-bfa7-a874668f45d5\")\n",
    "\n",
    "pc.create_index(\n",
    "  name=\"langchainvector\",\n",
    "  dimension= 1536,\n",
    "  metric=\"cosine\",\n",
    "  spec=PodSpec(\n",
    "    environment=\"us-west-1-gcp\",\n",
    "    pod_type=\"p1.x1\",\n",
    "    pods=1,\n",
    "    source_collection=\"example-collection\"\n",
    "  )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone.init(\n",
    "    api_key= \"aa77f6f7-bea1-4157-bfa7-a874668f45d5\",\n",
    "    environment = \"\"\n",
    ")\n",
    "index_name = \"langchainvector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
